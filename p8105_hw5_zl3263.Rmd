---
title: "p8105_hw5_zl3263"
output: github_document
date: "2022-11-15"
---

```{r}
library(tidyverse)
library(purrr)
```

### Problem 2

```{r}
homicides = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")%>%
   janitor::clean_names()
  
```

**Description**

The dataset record the homicides  information of 50 U.S cities. It contains `r nrow(homicides_raw)` observations and `r ncol(homicides_raw)` variables.The key variables include victims' name, sex, age and race. The location(`city`,`state`,`lat`,`lon`) and report time are also recorded.

**Count unsolved numbers**

```{r}
homicides = homicides %>%
  mutate(
    city_state = paste(city,",",state)
  )

solve_count = homicides %>%
  group_by(city_state) %>%
  summarise(
    total = n(),
    unsolved = sum(disposition == "Closed without arrest") + sum(disposition == "Open/No arrest")
    
  )

solve_count
```
**Proportion test on Baltimore**

```{r}
test_baltimore = prop.test(1825, 2827)%>%
  broom::tidy()
test_baltimore
```
**Proportion test on all the cities**

```{r}
solve_count = 
solve_count %>%
  mutate(
    prop_test = map2(.x = unsolved, .y = total, ~ broom::tidy(prop.test(.x, .y)))
  ) %>%
  unnest(cols = prop_test) %>%
  select(-c("parameter","method","alternative"))

head(solve_count)
```

**Plot**
```{r}
solve_count %>%
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
	  geom_point() +
	  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
	  labs(
	  	x = "Cities",
	  	y = "Estimates with Confidence Intervals",
	  	title = "Proportion of unsolved cases"
	  ) +
	  theme(
	  	axis.text.x = element_text(angle = 90)
	  )

  
```

### Problem 3

**generate simulation data**
```{r, cache=TRUE}
simtest_muhat = function(mu, sigma = 5, n = 30){
  
  x = rnorm(n = n, mean = mu, sd = sigma)
  
  result = t.test(x,mu = 0) %>%
    broom::tidy()%>%
    select(estimate,p.value)
  
  
  return(result)
}

sim_results_df = 
  expand_grid(
    mu = c(0, 1, 2, 3, 4, 5),
    iter = 1:5000
  ) %>% 
  mutate(
    test_coef = map(mu, simtest_muhat)
  ) %>% 
  unnest(test_coef)

```

**Plot**

```{r}
sim_results_df %>%
  mutate(
    reject = ifelse(p.value < 0.05, TRUE, FALSE)
  )%>%
  group_by(mu) %>%
  summarise(
    rej_r = sum(reject)/n()
  ) %>%
  ggplot(aes(x = mu, y = rej_r)) +
  geom_line() +
  labs(
    x = "μ",
	  y = "Reject Proportion",
  )
```

The plot shows the rejected proportion of each $\mu$ at h0: $\mu = 0$  to be tested. According to the plot, the effect size have positive relationship with the power of test.

```{r}
sim_results_df %>%
  mutate(
    reject = ifelse(p.value < 0.05, TRUE, FALSE)
  ) %>%
  group_by(mu) %>%
  summarise(
    mean_muhat = mean(estimate),
    mean_murej = sum(estimate*as.numeric(reject))/sum(reject)
  ) %>%
  pivot_longer(
    mean_muhat:mean_murej,
    names_to = "type",
    values_to = "mean_mu"
  )%>%
  ggplot(aes(x = mu, y = mean_mu, color = type)) +
  geom_line()+
  labs(
    x = "True μ",
	  y = "Average μ hat",
  )
```
When the true $\mu$ is close but not equal to the $\mu$ value (0) in H0, the average of $\hat{\mu} $ is away from the true $\mu$, because when $\hat{\mu}$ is close enough to 0 test fail to reject H0. As the true  $\mu$ gets more and more faraway from the $\mu$ to be tested, in fewer case the "fail to reject" happens and $\hat{\mu}$ get closer to true $\mu$